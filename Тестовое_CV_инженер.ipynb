{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Тестовое задание на Junior CV Engineer\n",
        "\n",
        "Главное — чтобы задачи решались, и сам процесс был полезным (а ещё лучше — интересным) для всех. Поэтому предлагаем два варианта проекта:\n",
        "\n",
        "1. Проект по классификации наличия святого Георгия на изображении. В папке находятся два файла со списками изображений: \"Георгиев\" и \"не Георгиев\". Необходимо создать Jupyter Notebook, в котором будет обучаться модель для классификации изображений по этим двум категориям. Скачать файлы можно с помощью команды `wget --random-wait -i filename.txt`.  \n"
      ],
      "metadata": {
        "id": "Qhk4ERD0XFpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qXorBVNe1tj",
        "outputId": "07e18b1a-6269-48c4-9700-d231c1930c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.12/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import os\n",
        "import wget\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from PIL import Image, ImageOps\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "7tq7euOsaZ0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем пути"
      ],
      "metadata": {
        "id": "tNOvC4W-eZR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "general_path =Path('/content/drive/MyDrive/test_assigment')"
      ],
      "metadata": {
        "id": "vFISSZHWZXyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "georges_path = general_path /'georges.csv'"
      ],
      "metadata": {
        "id": "PMFLqD3XZq7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_georges_path =general_path /'non_georges.csv'"
      ],
      "metadata": {
        "id": "wV-w9CNRbpPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтение данных и создание выброк  Мы имеем ограниченное время и возьмем часть фалов"
      ],
      "metadata": {
        "id": "h5tZNSc7fLJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Читаем CSV-файлы\n",
        "georges_links = pd.read_csv(georges_path, header=None)[0].tolist()\n",
        "not_georges_links = pd.read_csv(non_georges_path, header=None)[0].tolist()"
      ],
      "metadata": {
        "id": "F0cPo-utfEb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Возьмем по 2000 примеров\n",
        "georges_links = georges_links[:2000]\n",
        "not_georges_links = not_georges_links[:2000]\n"
      ],
      "metadata": {
        "id": "6fGQwdKpfaod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание выборок\n",
        "Первый шаг: Разделили данные на тренировочную выборку и временную выборку (temp_georges) в соотношении 70% / 30%\n",
        "Второй шаг: Разделили временную выборку (temp_georges) на валидационную и тестовую выборки в соотношении 50% / 50%"
      ],
      "metadata": {
        "id": "W2x2QNgff4My"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_georges, temp_georges = train_test_split(georges_links, test_size=0.3, random_state=42)\n",
        "val_georges, test_georges = train_test_split(temp_georges, test_size=0.5, random_state=42)\n",
        "\n",
        "train_not_georges, temp_not_georges = train_test_split(not_georges_links, test_size=0.3, random_state=42)\n",
        "val_not_georges, test_not_georges = train_test_split(temp_not_georges, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "CAaQbZlHf6tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для скачивания изображений\n",
        "def download_images(links, folder):\n",
        "    for link in links:\n",
        "        try:\n",
        "            filename = wget.download(link, out=folder)\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при скачивании {link}: {e}\")\n",
        "\n",
        "# Создаем папки\n",
        "os.makedirs('/content/data/train/georges', exist_ok=True)\n",
        "os.makedirs('/content/data/train/not_georges', exist_ok=True)\n",
        "os.makedirs('/content/data/val/georges', exist_ok=True)\n",
        "os.makedirs('/content/data/val/not_georges', exist_ok=True)\n",
        "os.makedirs('/content/data/test/georges', exist_ok=True)\n",
        "os.makedirs('/content/data/test/not_georges', exist_ok=True)\n",
        "\n",
        "# Скачиваем изображения\n",
        "download_images(train_georges, '/content/data/train/georges')\n",
        "download_images(val_georges, '/content/data/val/georges')\n",
        "download_images(test_georges, '/content/data/test/georges')\n",
        "\n",
        "download_images(train_not_georges, '/content/data/train/not_georges')\n",
        "download_images(val_not_georges, '/content/data/val/not_georges')\n",
        "download_images(test_not_georges, '/content/data/test/not_georges')"
      ],
      "metadata": {
        "id": "oF6ruVpYg7DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка изображений"
      ],
      "metadata": {
        "id": "mDtA1mkDiC56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_images(folder_path):\n",
        "    \"\"\"\n",
        "    Проверяет изображения в папке на повреждения.\n",
        "    Args:\n",
        "        folder_path (str): Путь к папке с изображениями.\n",
        "    Returns:\n",
        "        dict: Отчет о проверке (количество файлов, поврежденных файлов).\n",
        "    \"\"\"\n",
        "    total_files = 0\n",
        "    corrupted_files = 0\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        total_files += 1\n",
        "\n",
        "        try:\n",
        "            # Попытка открыть изображение\n",
        "            img = Image.open(file_path)\n",
        "            img.verify()  # Проверка целостности файла\n",
        "            img.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Поврежденный файл {file_path}: {e}\")\n",
        "            corrupted_files += 1\n",
        "            # Удаляем поврежденный файл\n",
        "            os.remove(file_path)\n",
        "\n",
        "    print(f\"Проверено файлов: {total_files}, Поврежденных файлов: {corrupted_files}\")\n",
        "    return {\"total_files\": total_files, \"corrupted_files\": corrupted_files}\n",
        "\n",
        "#Применение для всех папок\n",
        "folders_to_check = [\n",
        "    '/content/data/train/georges',\n",
        "    '/content/data/train/not_georges',\n",
        "    '/content/data/val/georges',\n",
        "    '/content/data/val/not_georges'\n",
        "]\n",
        "\n",
        "for folder in folders_to_check:\n",
        "    print(f\"\\nПроверка изображений в папке: {folder}\")\n",
        "    report = check_images(folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0KZKot8sq3C",
        "outputId": "f2390caf-b815-4481-fbe5-c04b2c06269e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Проверка изображений в папке: /content/data/train/georges\n",
            "Проверено файлов: 1400, Поврежденных файлов: 0\n",
            "\n",
            "Проверка изображений в папке: /content/data/train/not_georges\n",
            "Проверено файлов: 1400, Поврежденных файлов: 0\n",
            "\n",
            "Проверка изображений в папке: /content/data/val/georges\n",
            "Проверено файлов: 300, Поврежденных файлов: 0\n",
            "\n",
            "Проверка изображений в папке: /content/data/val/not_georges\n",
            "Проверено файлов: 300, Поврежденных файлов: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_images(image_paths, labels):\n",
        "    \"\"\"\n",
        "    Проверяет все изображения на корректность: существование, размер, формат, нормализацию.\n",
        "    Args:\n",
        "        image_paths (list): Список путей к изображениям.\n",
        "        labels (list): Список меток классов.\n",
        "    Returns:\n",
        "        dict: Отчет о проверке (количество файлов, поврежденных файлов, ошибок).\n",
        "    \"\"\"\n",
        "    total_files = len(image_paths)\n",
        "    corrupted_files = 0\n",
        "    size_errors = 0\n",
        "    normalization_errors = 0\n",
        "\n",
        "    for path, label in zip(image_paths, labels):\n",
        "        try:\n",
        "            # Чтение файла\n",
        "            image = tf.io.read_file(path)\n",
        "            image = tf.image.decode_jpeg(image, channels=3)  # Декодирование JPEG\n",
        "\n",
        "            # Проверка размера\n",
        "            if image.shape[:2] != (128, 128):\n",
        "                image = tf.image.resize(image, [128, 128])\n",
        "                size_errors += 1\n",
        "\n",
        "            # Нормализация\n",
        "            image /= 255.0\n",
        "            if tf.reduce_min(image).numpy() < 0 or tf.reduce_max(image).numpy() > 1:\n",
        "                normalization_errors += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при проверке файла {path}: {e}\")\n",
        "            corrupted_files += 1\n",
        "\n",
        "    print(f\"Проверено файлов: {total_files}\")\n",
        "    print(f\"Поврежденных файлов: {corrupted_files}\")\n",
        "    print(f\"Файлов с некорректным размером: {size_errors}\")\n",
        "    print(f\"Файлов с ошибками нормализации: {normalization_errors}\")\n",
        "\n",
        "    return {\n",
        "        \"total_files\": total_files,\n",
        "        \"corrupted_files\": corrupted_files,\n",
        "        \"size_errors\": size_errors,\n",
        "        \"normalization_errors\": normalization_errors\n",
        "    }"
      ],
      "metadata": {
        "id": "MdEaymhxvSkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "создание Tensorflow- совместимого датасета с использованием tf.data\n",
        "Приведениек стандартному размеру"
      ],
      "metadata": {
        "id": "lDMMozqRtHci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_image(path, label):\n",
        "    \"\"\"\n",
        "    Загружает и предобрабатывает изображение: чтение, изменение размера и нормализация.\n",
        "    Args:\n",
        "        path (str): Путь к файлу изображения.\n",
        "        label (int): Метка класса (1 - Георгий, 0 - не Георгий).\n",
        "    Returns:\n",
        "        tuple: Кортеж (изображение, метка), где изображение нормализовано и имеет размер 128x128.\n",
        "    \"\"\"\n",
        "    image = tf.io.read_file(path)  # Чтение файла изображения\n",
        "    image = tf.image.decode_jpeg(image, channels=3)  # Декодирование JPEG в RGB\n",
        "    image = tf.image.resize(image, [128, 128])  # Изменение размера до 128x128\n",
        "    image /= 255.0  # Нормализация значений пикселей в диапазон [0, 1]\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def create_dataset(image_paths, labels, batch_size=64):\n",
        "    \"\"\"\n",
        "    Создает TensorFlow-совместимый датасет из путей к изображениям и меток.\n",
        "    Args:\n",
        "        image_paths (list): Список путей к изображениям.\n",
        "        labels (list): Список меток классов.\n",
        "        batch_size (int): Размер батча.\n",
        "    Returns:\n",
        "        tf.data.Dataset: Оптимизированный датасет с перемешиванием, батчами и предзагрузкой.\n",
        "    \"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))  # Создание датасета\n",
        "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)  # Применение предобработки\n",
        "    dataset = dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)  # Оптимизация\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Пути к изображениям\n",
        "train_paths = [\n",
        "    '/content/data/train/georges/' + f for f in os.listdir('/content/data/train/georges')\n",
        "] + [\n",
        "    '/content/data/train/not_georges/' + f for f in os.listdir('/content/data/train/not_georges')\n",
        "]\n",
        "train_labels = [1] * len(os.listdir('/content/data/train/georges')) + \\\n",
        "               [0] * len(os.listdir('/content/data/train/not_georges'))\n",
        "\"\"\"\n",
        "Создает списки путей и меток для тренировочной выборки.\n",
        "\"\"\"\n",
        "\n",
        "val_paths = [\n",
        "    '/content/data/val/georges/' + f for f in os.listdir('/content/data/val/georges')\n",
        "] + [\n",
        "    '/content/data/val/not_georges/' + f for f in os.listdir('/content/data/val/not_georges')\n",
        "]\n",
        "val_labels = [1] * len(os.listdir('/content/data/val/georges')) + \\\n",
        "             [0] * len(os.listdir('/content/data/val/not_georges'))\n",
        "\"\"\"\n",
        "Создает списки путей и меток для валидационной выборки.\n",
        "\"\"\"\n",
        "\n",
        "# Создаем датасеты\n",
        "train_dataset = create_dataset(train_paths, train_labels)\n",
        "val_dataset = create_dataset(val_paths, val_labels)\n",
        "\n",
        "# Отчет о создании датасетов\n",
        "print(\"\\n=== Отчет о создании датасетов ===\")\n",
        "print(f\"Тренировочный датасет создан: {len(train_paths)} изображений приведены к размеру 128x128 и нормализованы.\")\n",
        "print(f\"Валидационный датасет создан: {len(val_paths)} изображений приведены к размеру 128x128 и нормализованы.\")"
      ],
      "metadata": {
        "id": "e4en0bbrnh_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec6dedf8-6931-4f12-a4fe-ae58d654c45f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Отчет о создании датасетов ===\n",
            "Тренировочный датасет создан: 2800 изображений приведены к размеру 128x128 и нормализованы.\n",
            "Валидационный датасет создан: 600 изображений приведены к размеру 128x128 и нормализованы.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка тренировочных данных\n",
        "print(\"\\n=== Проверка тренировочных данных ===\")\n",
        "train_report = validate_images(train_paths, train_labels)\n",
        "\n",
        "# Проверка валидационных данных\n",
        "print(\"\\n=== Проверка валидационных данных ===\")\n",
        "val_report = validate_images(val_paths, val_labels)\n",
        "\n",
        "# Вывод итогового отчета\n",
        "print(\"\\n=== Итоговый отчет ===\")\n",
        "print(\"Тренировочные данные:\")\n",
        "print(f\"  Всего файлов: {train_report['total_files']}\")\n",
        "print(f\"  Поврежденных файлов: {train_report['corrupted_files']}\")\n",
        "print(f\"  Файлов с некорректным размером: {train_report['size_errors']}\")\n",
        "print(f\"  Файлов с ошибками нормализации: {train_report['normalization_errors']}\")\n",
        "\n",
        "print(\"\\nВалидационные данные:\")\n",
        "print(f\"  Всего файлов: {val_report['total_files']}\")\n",
        "print(f\"  Поврежденных файлов: {val_report['corrupted_files']}\")\n",
        "print(f\"  Файлов с некорректным размером: {val_report['size_errors']}\")\n",
        "print(f\"  Файлов с ошибками нормализации: {val_report['normalization_errors']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQKlWNnF3Ygy",
        "outputId": "f23aeb05-b8d1-4f56-ce0c-b28465fd7a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Проверка тренировочных данных ===\n",
            "Проверено файлов: 2800\n",
            "Поврежденных файлов: 0\n",
            "Файлов с некорректным размером: 2800\n",
            "Файлов с ошибками нормализации: 0\n",
            "\n",
            "=== Проверка валидационных данных ===\n",
            "Проверено файлов: 600\n",
            "Поврежденных файлов: 0\n",
            "Файлов с некорректным размером: 600\n",
            "Файлов с ошибками нормализации: 0\n",
            "\n",
            "=== Итоговый отчет ===\n",
            "Тренировочные данные:\n",
            "  Всего файлов: 2800\n",
            "  Поврежденных файлов: 0\n",
            "  Файлов с некорректным размером: 2800\n",
            "  Файлов с ошибками нормализации: 0\n",
            "\n",
            "Валидационные данные:\n",
            "  Всего файлов: 600\n",
            "  Поврежденных файлов: 0\n",
            "  Файлов с некорректным размером: 600\n",
            "  Файлов с ошибками нормализации: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем предобученную модель"
      ],
      "metadata": {
        "id": "0QkuWyGWy_ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_resnet_model(input_shape=(128, 128, 3), num_classes=1):\n",
        "    \"\"\"\n",
        "    Создает модель на основе ResNet50 для бинарной классификации.\n",
        "    Args:\n",
        "        input_shape (tuple): Размер входных изображений (высота, ширина, каналы).\n",
        "        num_classes (int): Количество классов (1 для бинарной классификации).\n",
        "    Returns:\n",
        "        tf.keras.Model: Скомпилированная модель.\n",
        "    \"\"\"\n",
        "    # Загрузка предобученной модели ResNet50 с замороженными слоями\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False  # Замораживаем слои ResNet50\n",
        "\n",
        "    # Создание новой модели поверх ResNet50\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        GlobalAveragePooling2D(),  # Уменьшаем размерность выхода ResNet50\n",
        "        Dense(128, activation='relu'),  # Полносвязный слой\n",
        "        Dense(num_classes, activation='sigmoid')  # Выходной слой для бинарной классификации\n",
        "    ])\n",
        "\n",
        "    # Компиляция модели\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "990KS7-5zC8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создани и обучение модели"
      ],
      "metadata": {
        "id": "SzkU-D17zIYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем модель\n",
        "model = create_resnet_model()\n",
        "\n",
        "# Выводим сводку модели\n",
        "model.summary()\n",
        "\n",
        "# Обучение модели\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=30,  # Начнем с 30 эпох\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]  # ранняя остановка\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lFRpdXYJzNwJ",
        "outputId": "ea788ad7-878b-41a3-ceaa-dd7325b6ca46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m262,272\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,850,113\u001b[0m (90.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,850,113</span> (90.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m262,401\u001b[0m (1.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,401</span> (1.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 571ms/step - accuracy: 0.7704 - loss: 0.6613 - val_accuracy: 0.5000 - val_loss: 1.0821\n",
            "Epoch 2/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 328ms/step - accuracy: 0.2348 - loss: 1.2231 - val_accuracy: 0.5000 - val_loss: 0.7685\n",
            "Epoch 3/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 348ms/step - accuracy: 0.2032 - loss: 0.9427 - val_accuracy: 0.5000 - val_loss: 0.7241\n",
            "Epoch 4/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 327ms/step - accuracy: 0.2017 - loss: 0.8560 - val_accuracy: 0.5000 - val_loss: 0.7187\n",
            "Epoch 5/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 330ms/step - accuracy: 0.2025 - loss: 0.8390 - val_accuracy: 0.5000 - val_loss: 0.7044\n",
            "Epoch 6/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 333ms/step - accuracy: 0.2038 - loss: 0.8060 - val_accuracy: 0.5000 - val_loss: 0.7034\n",
            "Epoch 7/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 350ms/step - accuracy: 0.2066 - loss: 0.7991 - val_accuracy: 0.5000 - val_loss: 0.6978\n",
            "Epoch 8/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 331ms/step - accuracy: 0.2032 - loss: 0.7827 - val_accuracy: 0.5000 - val_loss: 0.6952\n",
            "Epoch 9/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 330ms/step - accuracy: 0.2212 - loss: 0.7724 - val_accuracy: 0.5050 - val_loss: 0.6918\n",
            "Epoch 10/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 326ms/step - accuracy: 0.2337 - loss: 0.7591 - val_accuracy: 0.5050 - val_loss: 0.6903\n",
            "Epoch 11/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 318ms/step - accuracy: 0.2305 - loss: 0.7543 - val_accuracy: 0.5100 - val_loss: 0.6885\n",
            "Epoch 12/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 337ms/step - accuracy: 0.2315 - loss: 0.7472 - val_accuracy: 0.5133 - val_loss: 0.6874\n",
            "Epoch 13/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 329ms/step - accuracy: 0.2452 - loss: 0.7383 - val_accuracy: 0.5217 - val_loss: 0.6865\n",
            "Epoch 14/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 324ms/step - accuracy: 0.2470 - loss: 0.7347 - val_accuracy: 0.5217 - val_loss: 0.6866\n",
            "Epoch 15/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 325ms/step - accuracy: 0.2506 - loss: 0.7333 - val_accuracy: 0.5317 - val_loss: 0.6858\n",
            "Epoch 16/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 329ms/step - accuracy: 0.2559 - loss: 0.7301 - val_accuracy: 0.5417 - val_loss: 0.6852\n",
            "Epoch 17/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 367ms/step - accuracy: 0.2598 - loss: 0.7253 - val_accuracy: 0.5467 - val_loss: 0.6849\n",
            "Epoch 18/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 328ms/step - accuracy: 0.2766 - loss: 0.7227 - val_accuracy: 0.5433 - val_loss: 0.6847\n",
            "Epoch 19/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 323ms/step - accuracy: 0.2738 - loss: 0.7227 - val_accuracy: 0.5450 - val_loss: 0.6844\n",
            "Epoch 20/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 327ms/step - accuracy: 0.2735 - loss: 0.7220 - val_accuracy: 0.5600 - val_loss: 0.6837\n",
            "Epoch 21/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 327ms/step - accuracy: 0.3064 - loss: 0.7163 - val_accuracy: 0.5583 - val_loss: 0.6839\n",
            "Epoch 22/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 315ms/step - accuracy: 0.2894 - loss: 0.7164 - val_accuracy: 0.5600 - val_loss: 0.6835\n",
            "Epoch 23/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 331ms/step - accuracy: 0.3008 - loss: 0.7167 - val_accuracy: 0.5633 - val_loss: 0.6837\n",
            "Epoch 24/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 330ms/step - accuracy: 0.2938 - loss: 0.7136 - val_accuracy: 0.5617 - val_loss: 0.6833\n",
            "Epoch 25/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 331ms/step - accuracy: 0.3040 - loss: 0.7126 - val_accuracy: 0.5650 - val_loss: 0.6827\n",
            "Epoch 26/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 330ms/step - accuracy: 0.3290 - loss: 0.7108 - val_accuracy: 0.5633 - val_loss: 0.6825\n",
            "Epoch 27/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 318ms/step - accuracy: 0.3233 - loss: 0.7114 - val_accuracy: 0.5750 - val_loss: 0.6820\n",
            "Epoch 28/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 329ms/step - accuracy: 0.3376 - loss: 0.7071 - val_accuracy: 0.5717 - val_loss: 0.6812\n",
            "Epoch 29/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 317ms/step - accuracy: 0.3225 - loss: 0.7076 - val_accuracy: 0.5733 - val_loss: 0.6816\n",
            "Epoch 30/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 333ms/step - accuracy: 0.3484 - loss: 0.7085 - val_accuracy: 0.5733 - val_loss: 0.6808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разморозка дополнительных  слоев и  дообучение"
      ],
      "metadata": {
        "id": "V61z2OfnB2si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.trainable = True\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_fine = model.fit(train_dataset, validation_data=val_dataset, epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjdmJofUB66y",
        "outputId": "655a62b9-6b57-47f2-909e-00e235ec5f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 525ms/step - accuracy: 0.3661 - loss: 0.7022 - val_accuracy: 0.5733 - val_loss: 0.6805\n",
            "Epoch 2/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 327ms/step - accuracy: 0.3736 - loss: 0.7018 - val_accuracy: 0.5750 - val_loss: 0.6803\n",
            "Epoch 3/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 370ms/step - accuracy: 0.3699 - loss: 0.7017 - val_accuracy: 0.5750 - val_loss: 0.6800\n",
            "Epoch 4/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 329ms/step - accuracy: 0.3822 - loss: 0.7002 - val_accuracy: 0.5817 - val_loss: 0.6797\n",
            "Epoch 5/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 347ms/step - accuracy: 0.3862 - loss: 0.7001 - val_accuracy: 0.5817 - val_loss: 0.6795\n",
            "Epoch 6/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 330ms/step - accuracy: 0.3649 - loss: 0.7019 - val_accuracy: 0.5800 - val_loss: 0.6792\n",
            "Epoch 7/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 315ms/step - accuracy: 0.3803 - loss: 0.6996 - val_accuracy: 0.5750 - val_loss: 0.6789\n",
            "Epoch 8/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 330ms/step - accuracy: 0.3944 - loss: 0.6976 - val_accuracy: 0.5767 - val_loss: 0.6786\n",
            "Epoch 9/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 316ms/step - accuracy: 0.3899 - loss: 0.6974 - val_accuracy: 0.5783 - val_loss: 0.6783\n",
            "Epoch 10/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 330ms/step - accuracy: 0.4015 - loss: 0.6969 - val_accuracy: 0.5817 - val_loss: 0.6780\n",
            "Epoch 11/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 330ms/step - accuracy: 0.3971 - loss: 0.6977 - val_accuracy: 0.5817 - val_loss: 0.6777\n",
            "Epoch 12/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 349ms/step - accuracy: 0.4118 - loss: 0.6956 - val_accuracy: 0.5800 - val_loss: 0.6774\n",
            "Epoch 13/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 328ms/step - accuracy: 0.4210 - loss: 0.6927 - val_accuracy: 0.5800 - val_loss: 0.6771\n",
            "Epoch 14/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 321ms/step - accuracy: 0.4202 - loss: 0.6934 - val_accuracy: 0.5833 - val_loss: 0.6768\n",
            "Epoch 15/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 367ms/step - accuracy: 0.4327 - loss: 0.6926 - val_accuracy: 0.5850 - val_loss: 0.6766\n",
            "Epoch 16/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 319ms/step - accuracy: 0.4180 - loss: 0.6946 - val_accuracy: 0.5867 - val_loss: 0.6763\n",
            "Epoch 17/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 369ms/step - accuracy: 0.4389 - loss: 0.6921 - val_accuracy: 0.5883 - val_loss: 0.6761\n",
            "Epoch 18/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 363ms/step - accuracy: 0.4309 - loss: 0.6928 - val_accuracy: 0.5917 - val_loss: 0.6759\n",
            "Epoch 19/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 328ms/step - accuracy: 0.4449 - loss: 0.6907 - val_accuracy: 0.5900 - val_loss: 0.6756\n",
            "Epoch 20/20\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 337ms/step - accuracy: 0.4445 - loss: 0.6918 - val_accuracy: 0.5917 - val_loss: 0.6754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Продолжим обучение"
      ],
      "metadata": {
        "id": "ToNlmNbeDe0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.trainable = True\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_fine = model.fit(train_dataset, validation_data=val_dataset, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA90xiI9DhgZ",
        "outputId": "c43b6771-fcdb-4a9a-98a7-e2e4b865e77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 528ms/step - accuracy: 0.4528 - loss: 0.6891 - val_accuracy: 0.5950 - val_loss: 0.6752\n",
            "Epoch 2/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 332ms/step - accuracy: 0.4750 - loss: 0.6866 - val_accuracy: 0.5933 - val_loss: 0.6751\n",
            "Epoch 3/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 332ms/step - accuracy: 0.4695 - loss: 0.6875 - val_accuracy: 0.5967 - val_loss: 0.6750\n",
            "Epoch 4/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 331ms/step - accuracy: 0.4621 - loss: 0.6880 - val_accuracy: 0.5950 - val_loss: 0.6748\n",
            "Epoch 5/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 330ms/step - accuracy: 0.4675 - loss: 0.6868 - val_accuracy: 0.5933 - val_loss: 0.6747\n",
            "Epoch 6/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 331ms/step - accuracy: 0.4641 - loss: 0.6862 - val_accuracy: 0.5950 - val_loss: 0.6745\n",
            "Epoch 7/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 333ms/step - accuracy: 0.4524 - loss: 0.6893 - val_accuracy: 0.5967 - val_loss: 0.6744\n",
            "Epoch 8/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 327ms/step - accuracy: 0.4608 - loss: 0.6874 - val_accuracy: 0.6017 - val_loss: 0.6743\n",
            "Epoch 9/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 334ms/step - accuracy: 0.4631 - loss: 0.6867 - val_accuracy: 0.6067 - val_loss: 0.6741\n",
            "Epoch 10/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 360ms/step - accuracy: 0.4652 - loss: 0.6864 - val_accuracy: 0.6083 - val_loss: 0.6740\n",
            "Epoch 11/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 330ms/step - accuracy: 0.4726 - loss: 0.6862 - val_accuracy: 0.6083 - val_loss: 0.6739\n",
            "Epoch 12/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 361ms/step - accuracy: 0.4673 - loss: 0.6859 - val_accuracy: 0.6050 - val_loss: 0.6738\n",
            "Epoch 13/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 318ms/step - accuracy: 0.4708 - loss: 0.6855 - val_accuracy: 0.6067 - val_loss: 0.6736\n",
            "Epoch 14/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 327ms/step - accuracy: 0.4800 - loss: 0.6841 - val_accuracy: 0.6067 - val_loss: 0.6735\n",
            "Epoch 15/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 342ms/step - accuracy: 0.4744 - loss: 0.6854 - val_accuracy: 0.6067 - val_loss: 0.6734\n",
            "Epoch 16/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 315ms/step - accuracy: 0.4853 - loss: 0.6831 - val_accuracy: 0.6067 - val_loss: 0.6733\n",
            "Epoch 17/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 372ms/step - accuracy: 0.4853 - loss: 0.6824 - val_accuracy: 0.6083 - val_loss: 0.6731\n",
            "Epoch 18/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 323ms/step - accuracy: 0.4893 - loss: 0.6836 - val_accuracy: 0.6050 - val_loss: 0.6730\n",
            "Epoch 19/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 355ms/step - accuracy: 0.4930 - loss: 0.6820 - val_accuracy: 0.6050 - val_loss: 0.6729\n",
            "Epoch 20/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 369ms/step - accuracy: 0.4773 - loss: 0.6833 - val_accuracy: 0.6033 - val_loss: 0.6728\n",
            "Epoch 21/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 332ms/step - accuracy: 0.4952 - loss: 0.6816 - val_accuracy: 0.6050 - val_loss: 0.6727\n",
            "Epoch 22/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 323ms/step - accuracy: 0.4995 - loss: 0.6816 - val_accuracy: 0.6083 - val_loss: 0.6726\n",
            "Epoch 23/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 351ms/step - accuracy: 0.4950 - loss: 0.6825 - val_accuracy: 0.6117 - val_loss: 0.6725\n",
            "Epoch 24/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 320ms/step - accuracy: 0.4984 - loss: 0.6794 - val_accuracy: 0.6117 - val_loss: 0.6723\n",
            "Epoch 25/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 329ms/step - accuracy: 0.5005 - loss: 0.6802 - val_accuracy: 0.6150 - val_loss: 0.6723\n",
            "Epoch 26/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 340ms/step - accuracy: 0.5096 - loss: 0.6791 - val_accuracy: 0.6150 - val_loss: 0.6722\n",
            "Epoch 27/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 327ms/step - accuracy: 0.5017 - loss: 0.6808 - val_accuracy: 0.6117 - val_loss: 0.6721\n",
            "Epoch 28/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 338ms/step - accuracy: 0.5157 - loss: 0.6784 - val_accuracy: 0.6133 - val_loss: 0.6720\n",
            "Epoch 29/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 322ms/step - accuracy: 0.5026 - loss: 0.6806 - val_accuracy: 0.6117 - val_loss: 0.6719\n",
            "Epoch 30/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 326ms/step - accuracy: 0.5125 - loss: 0.6777 - val_accuracy: 0.6133 - val_loss: 0.6718\n",
            "Epoch 31/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 322ms/step - accuracy: 0.4979 - loss: 0.6801 - val_accuracy: 0.6133 - val_loss: 0.6717\n",
            "Epoch 32/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 365ms/step - accuracy: 0.5069 - loss: 0.6786 - val_accuracy: 0.6133 - val_loss: 0.6717\n",
            "Epoch 33/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 329ms/step - accuracy: 0.5004 - loss: 0.6798 - val_accuracy: 0.6117 - val_loss: 0.6716\n",
            "Epoch 34/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 328ms/step - accuracy: 0.5051 - loss: 0.6788 - val_accuracy: 0.6133 - val_loss: 0.6715\n",
            "Epoch 35/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 366ms/step - accuracy: 0.5148 - loss: 0.6776 - val_accuracy: 0.6133 - val_loss: 0.6714\n",
            "Epoch 36/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 328ms/step - accuracy: 0.5094 - loss: 0.6780 - val_accuracy: 0.6083 - val_loss: 0.6713\n",
            "Epoch 37/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 318ms/step - accuracy: 0.5035 - loss: 0.6770 - val_accuracy: 0.6083 - val_loss: 0.6713\n",
            "Epoch 38/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 367ms/step - accuracy: 0.5158 - loss: 0.6770 - val_accuracy: 0.6067 - val_loss: 0.6712\n",
            "Epoch 39/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 315ms/step - accuracy: 0.5123 - loss: 0.6769 - val_accuracy: 0.6067 - val_loss: 0.6711\n",
            "Epoch 40/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 367ms/step - accuracy: 0.5206 - loss: 0.6758 - val_accuracy: 0.6067 - val_loss: 0.6710\n",
            "Epoch 41/50\n",
            "\u001b[1m10/44\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 303ms/step - accuracy: 0.3896 - loss: 0.6806"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение модели с ранней остановкой\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Перекомпиляция модели с меньшей скоростью обучения\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Дообучение модели\n",
        "history_fine = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=100,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "3GVSbwweIAzK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}